{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11491429,"sourceType":"datasetVersion","datasetId":7195383}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============== #\n#    READ ME     #\n# ============== #\n\n# --- Objective --- #\n# Our goal was to create a project that enhances the media consumption experience for everyone, including individuals with impairments. \n# By providing automated solutions like image descriptions and video transcripts,we aimed to make media more accessible and easier to engage with. \n# Whether for general convenience or accessibility needs,this project seeks to bridge gaps and improve how people interact with digital content.\n\n# --- Keep in mind --- #\n# There should be a dataset in the notebook created in order to test the AI called \"DATASETS\"\n# Said dataset are linked and called in all code cells, but the last one. \n\n\n##### ---------------------------------- ##### \n# ++  In each of the cells we have ++ #\n##### ---------------------------------- ##### \n#    Text summurizer, image caption, audio trascript  # \n#           Video transcript and summurizer        #\n#             A media analyzer Web cell        #\n## >> Which does all of the above with files that can be uploaded ##\n\n# ----  Key features:  ----  #\n# >Document Summarization: Automatically extracts text from PDFs and generates summaries using transformers, improving efficiency for large text processing.\n# >Image Captioning: Uses BLIP for generating accurate descriptions of images, beneficial for content tagging and accessibility.\n# >Video Transcription: Extracts audio from videos and uses Whisper for transcription, enabling quick access to video content.\n# >Video Summarization: Breaks down videos into key frames, generates captions, and creates an overall summary, efficiently capturing video content.\n\n# The system adapts to various content types, making it useful in fields like research, media, and education. \n# It integrates generative AI to streamline tasks and improve productivity.\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:26:52.979681Z","iopub.execute_input":"2025-04-20T21:26:52.980319Z","iopub.status.idle":"2025-04-20T21:26:52.984111Z","shell.execute_reply.started":"2025-04-20T21:26:52.980298Z","shell.execute_reply":"2025-04-20T21:26:52.983285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ==================== #\n#   Text Summarizer    #\n# ===================  #\n\n\n\n# Install necessary libraries\n!pip install PyPDF2 transformers > /dev/null 2>&1\n\n# Import libraries\nimport os\nimport PyPDF2\nfrom transformers import pipeline\nimport torch\n\n# Ensure proper parallelism settings\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Ensure CUDA debugging is enabled\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Function to extract text from PDF\ndef extract_text_from_pdf(pdf_path):\n    pdf_reader = PyPDF2.PdfReader(pdf_path)\n    text = \"\"\n    for page in pdf_reader.pages:\n        text += page.extract_text()\n    return text\n\n# Function to summarize text using Hugging Face's transformers\ndef summarize_text(text, max_length=130, min_length=30):\n    try:\n        # Try to load the model on GPU if available\n        device = 0 if torch.cuda.is_available() else -1\n        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", revision=\"a4f8f3e\", device=device)\n    except RuntimeError as e:\n        if \"CUDA error\" in str(e):\n            print(\"CUDA error encountered. Switching to CPU...\")\n            summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", revision=\"a4f8f3e\", device=-1)\n        else:\n            raise e\n\n    # Split the text into manageable chunks and summarize each\n    chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]  # Split text into chunks\n    summaries = []\n\n    for chunk in chunks:\n        input_length = len(chunk.split())  # Tokenize the chunk to estimate its length\n        dynamic_max_length = min(max_length, max(11, input_length - 1))  # Adjust max_length dynamically\n        dynamic_min_length = min(min_length, dynamic_max_length - 1)  # Ensure min_length is less than max_length\n\n        # Summarize the chunk\n        summary = summarizer(chunk, max_length=dynamic_max_length, min_length=dynamic_min_length, do_sample=False)[0]['summary_text']\n        summaries.append(summary)\n\n    return \" \".join(summaries)\n\n# Main function\ndef main():\n    # Define the file path here\n    pdf_path = \"/kaggle/input/datasettest/about-love.pdf\"  # Replace with your PDF file's actual path\n    \n    # Check if the file exists\n    if not os.path.exists(pdf_path):\n        print(f\"Error: The file at '{pdf_path}' does not exist. Please check the path and try again.\")\n        return\n\n    # Extract text from PDF\n    print(\"Extracting text from PDF...\")\n    text = extract_text_from_pdf(pdf_path)\n    \n    # Summarize the extracted text\n    print(\"Summarizing the text...\")\n    try:\n        summary = summarize_text(text)\n    except Exception as e:\n        print(f\"An error occurred during summarization: {e}\")\n        return\n\n    # Output the summary\n    print(\"\\nSummary:\")\n    print(summary)\n\n    # Save the summary to a file\n    output_path = \"summary.txt\"\n    with open(output_path, \"w\") as file:\n        file.write(summary)\n    print(f\"\\nSummary saved to '{output_path}'.\")\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:26:52.985516Z","iopub.execute_input":"2025-04-20T21:26:52.985796Z","iopub.status.idle":"2025-04-20T21:27:16.701458Z","shell.execute_reply.started":"2025-04-20T21:26:52.985772Z","shell.execute_reply":"2025-04-20T21:27:16.700526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ================= #\n#   Image Caption   #\n# ================= #\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import required libraries\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\n# Function to describe an image\ndef describe_image(image_path_or_url):\n    # Load BLIP model and processor\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n\n    # Load the image\n    if image_path_or_url.startswith(\"http\"):\n        image = Image.open(requests.get(image_path_or_url, stream=True).raw)\n    else:\n        image = Image.open(image_path_or_url)\n\n    # Preprocess the image and generate caption\n    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs)\n    caption = processor.decode(outputs[0], skip_special_tokens=True)\n\n    return caption\n\n# Example usage\nif __name__ == \"__main__\":\n    # Provide an image file path or URL\n    image_url = \"/kaggle/input/datasettest/image0-3.jpg\"  # Replace with your image URL or file path\n    print(\"Describing the image...\")\n    caption = describe_image(image_url)\n    print(\"Caption:\", caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:27:16.703150Z","iopub.execute_input":"2025-04-20T21:27:16.703407Z","iopub.status.idle":"2025-04-20T21:27:18.661236Z","shell.execute_reply.started":"2025-04-20T21:27:16.703386Z","shell.execute_reply":"2025-04-20T21:27:18.660423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ================================= #\n#   Video Transcript & Summarizer  #\n# ================================ #\n\n\n\n\n# Install necessary libraries\n!pip install moviepy openai-whisper transformers > /dev/null 2>&1\n\n\n# Import libraries\nimport os\nimport whisper\nimport torch\nfrom transformers import pipeline\nfrom moviepy.editor import VideoFileClip\n\n# Suppress ALSA warnings\nos.environ[\"XDG_RUNTIME_DIR\"] = \"/tmp/runtime-dir\"\nos.makedirs(os.environ[\"XDG_RUNTIME_DIR\"], exist_ok=True)\n\n# Ensure proper runtime environment\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Function to extract audio from video\ndef extract_audio_from_video(video_path, audio_path=\"temp_audio.wav\"):\n    try:\n        print(\"Extracting audio from video...\")\n        video_clip = VideoFileClip(video_path)\n        video_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n        return audio_path\n    except Exception as e:\n        print(f\"An error occurred during audio extraction: {e}\")\n        return None\n\n# Function to transcribe audio using Whisper\ndef transcribe_audio(audio_path, model_name=\"base\"):\n    try:\n        print(\"Transcribing audio...\")\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model = whisper.load_model(model_name, device=device)\n        print(f\"Device set to use {device}\")\n        result = model.transcribe(audio_path)\n        transcript = result[\"text\"]\n        return transcript\n    except Exception as e:\n        print(f\"An error occurred during transcription: {e}\")\n        return None\n\n# Function to summarize text using Hugging Face's transformers\ndef summarize_text(text, max_length=130, min_length=30):\n    try:\n        print(\"Summarizing text...\")\n        device = 0 if torch.cuda.is_available() else -1\n        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n        return summary[0]['summary_text']\n    except Exception as e:\n        print(f\"An error occurred during summarization: {e}\")\n        return None\n\n# Main function to summarize a video\ndef summarize_video(video_path):\n    # Extract audio from video\n    audio_path = extract_audio_from_video(video_path)\n    if not audio_path:\n        return\n    \n    # Transcribe the extracted audio\n    transcript = transcribe_audio(audio_path)\n    if not transcript:\n        return\n    \n    print(\"\\nTranscript:\")\n    print(transcript)\n    \n    # Summarize the transcript\n    summary = summarize_text(transcript)\n    if not summary:\n        return\n    \n    print(\"\\nSummary:\")\n    print(summary)\n\n    # Save the summary to a file\n    with open(\"video_summary.txt\", \"w\") as file:\n        file.write(summary)\n    print(\"\\nSummary saved to 'video_summary.txt'.\")\n\n# Run the summarizer\nif __name__ == \"__main__\":\n    # Replace `input()` with a hardcoded file path or use command-line arguments\n    video_path = \"/kaggle/input/datasettest/Life of a Doduo _ Pokearth.mp4\"  # Replace with the path to your video file\n    if not os.path.exists(video_path):\n        print(f\"Error: The file at '{video_path}' does not exist. Please check the path and try again.\")\n    else:\n        summarize_video(video_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:27:18.662221Z","iopub.execute_input":"2025-04-20T21:27:18.662873Z","iopub.status.idle":"2025-04-20T21:27:37.273702Z","shell.execute_reply.started":"2025-04-20T21:27:18.662845Z","shell.execute_reply":"2025-04-20T21:27:37.272867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ===================== #\n#    Audio Transcript   #\n# ===================== #\n\n\n# Install necessary libraries\n!pip install openai-whisper ffmpeg > /dev/null 2>&1\n\n# Import libraries\nimport os\nimport whisper\nimport torch  # Ensure torch is imported\nfrom moviepy.editor import AudioFileClip\n\n# Ensure proper runtime settings\nif \"XDG_RUNTIME_DIR\" not in os.environ:\n    os.environ[\"XDG_RUNTIME_DIR\"] = \"/tmp/runtime-dir\"\n    os.makedirs(os.environ[\"XDG_RUNTIME_DIR\"], exist_ok=True)\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Debugging for CUDA\n\n# Function to transcribe audio using Whisper\ndef transcribe_audio(audio_path, model_name=\"base\"):\n    try:\n        # Load the Whisper model\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model = whisper.load_model(model_name, device=device)\n        print(f\"Device set to use {device}\")\n\n        # Transcribe the audio file\n        print(\"Transcribing audio...\")\n        result = model.transcribe(audio_path)\n        transcript = result[\"text\"]\n        return transcript\n    except Exception as e:\n        print(f\"An error occurred during transcription: {e}\")\n        return None\n\n# Main function\ndef main():\n    # Define the audio file path directly\n    audio_path = \"/kaggle/input/datasettest/Monster (April Fools)  EPIC_ The Musical ANIMATIC.mp3\"  # Replace with your actual file path\n    \n    # Check if the file exists\n    if not os.path.exists(audio_path):\n        print(f\"Error: The file at '{audio_path}' does not exist. Please check the path and try again.\")\n        return\n    \n    # Transcribe the audio\n    transcript = transcribe_audio(audio_path)\n    \n    # Output the transcript\n    if transcript:\n        print(\"\\nTranscript:\")\n        print(transcript)\n\n        # Save the transcript to a file\n        output_path = \"transcript.txt\"\n        with open(output_path, \"w\") as file:\n            file.write(transcript)\n        print(f\"\\nTranscript saved to '{output_path}'.\")\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:27:37.275427Z","iopub.execute_input":"2025-04-20T21:27:37.275660Z","iopub.status.idle":"2025-04-20T21:27:42.255942Z","shell.execute_reply.started":"2025-04-20T21:27:37.275638Z","shell.execute_reply":"2025-04-20T21:27:42.255151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ================================== #\n#   AI Media Analyzer Web App Cell   #\n# ================================== #\n\n\n\n\n# Install dependencies\n!pip install -q gradio PyPDF2 transformers pillow torch openai-whisper moviepy\n\nimport gradio as gr\nimport os\nimport tempfile\n\nimport PyPDF2\nfrom transformers import pipeline, BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport whisper\nfrom moviepy.editor import VideoFileClip\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n\n# Function\ndef summarize_text(text, max_length=130, min_length=30):\n    device = 0 if torch.cuda.is_available() else -1\n    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=device)\n    chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]\n    summaries = []\n    for chunk in chunks:\n        input_length = len(chunk.split())\n        dynamic_max_length = min(max_length, max(11, input_length-1))\n        dynamic_min_length = min(min_length, dynamic_max_length-1)\n        summary = summarizer(chunk, max_length=dynamic_max_length, min_length=dynamic_min_length, do_sample=False)[0]['summary_text']\n        summaries.append(summary)\n    return \" \".join(summaries)\n\n# Main function for type of file #\ndef extract_text_from_pdf(pdf_path):\n    pdf_reader = PyPDF2.PdfReader(pdf_path)\n    text = \"\"\n    for page in pdf_reader.pages:\n        page_text = page.extract_text()\n        if page_text:\n            text += page_text\n    return text\n\ndef describe_image(image_path):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs)\n    caption = processor.decode(outputs[0], skip_special_tokens=True)\n    return caption\n\ndef transcribe_audio(audio_path, model_name=\"base\"):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = whisper.load_model(model_name, device=device)\n    result = model.transcribe(audio_path)\n    return result[\"text\"]\n\ndef extract_audio_from_video(video_path):\n    audio_temp = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n    video_clip = VideoFileClip(video_path)\n    video_clip.audio.write_audiofile(audio_temp.name, verbose=False, logger=None)\n    return audio_temp.name\n\ndef ai_media_analyzer(file_path):\n    if file_path is None:\n        return None, None, None, \"Error:\", \"No file provided!\"\n    file_ext = os.path.splitext(file_path)[-1].lower()\n\n    # PDF format\n    if file_ext == \".pdf\":\n        text = extract_text_from_pdf(file_path)\n        summary = summarize_text(text)\n        return None, None, None, \"Summary:\", summary\n\n    # Image format\n    elif file_ext in [\".jpg\", \".jpeg\", \".png\"]:\n        caption = describe_image(file_path)\n        return file_path, None, None, \"Caption:\", caption\n\n    # Audio format\n    elif file_ext in [\".mp3\", \".wav\"]:\n        transcript = transcribe_audio(file_path)\n        return None, file_path, None, \"Transcript:\", transcript\n\n    # Video format\n    elif file_ext in [\".mp4\", \".avi\", \".mov\"]:\n        audio_path = extract_audio_from_video(file_path)\n        transcript = transcribe_audio(audio_path)\n        summary = summarize_text(transcript)\n        return None, None, file_path, \"Transcript & Summary:\", f\"{transcript}\\n\\nSummary:\\n{summary}\"\n\n    else:\n        return None, None, None, \"Error:\", \"Unsupported file type.\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# AI Media Analyzer\\nUpload a PDF, image, audio, or video file. The AI will generate a transcript, caption, or summary as appropriate.\")\n\n    inp = gr.File(label=\"Upload a file\")\n    out_image = gr.Image(label=\"Image Preview\", visible=False)\n    out_audio = gr.Audio(label=\"Audio Preview\", visible=False)\n    out_video = gr.Video(label=\"Video Preview\", visible=False)\n    out_type = gr.Textbox(label=\"Analysis Type\")\n    out_result = gr.Textbox(label=\"AI Output\")\n\n    def analyze_and_preview(file):\n        if file is None:\n            return None, None, None, \"Error:\", \"No file provided!\"\n        img, aud, vid, type_, result = ai_media_analyzer(file)\n        # Set visibility based on file type\n        return (\n            img if img else None,\n            aud if aud else None,\n            vid if vid else None,\n            type_,\n            result\n        )\n\n    inp.change(\n        analyze_and_preview, \n        inputs=inp, \n        outputs=[out_image, out_audio, out_video, out_type, out_result]\n    )\n\ndemo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:27:42.256822Z","iopub.execute_input":"2025-04-20T21:27:42.257033Z","iopub.status.idle":"2025-04-20T21:27:46.701968Z","shell.execute_reply.started":"2025-04-20T21:27:42.257012Z","shell.execute_reply":"2025-04-20T21:27:46.701351Z"}},"outputs":[],"execution_count":null}]}